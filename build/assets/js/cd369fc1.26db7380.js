"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[852],{4280(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"capstone/index","title":"Autonomous Humanoid Robot Capstone Project","description":"Overview","source":"@site/docs/capstone/index.md","sourceDirName":"capstone","slug":"/capstone/","permalink":"/AI-Robotics-book/docs/capstone/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1768307371000,"frontMatter":{},"sidebar":"tutorialSidebar","next":{"title":"Why Physical AI Matters","permalink":"/AI-Robotics-book/docs/intro"}}');var s=i(4848),o=i(8453);const a={},r="Autonomous Humanoid Robot Capstone Project",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Project Structure",id:"project-structure",level:2},{value:"1. System Architecture Design",id:"1-system-architecture-design",level:3},{value:"Architecture Overview",id:"architecture-overview",level:4},{value:"2. Hardware/Software Requirements",id:"2-hardwaresoftware-requirements",level:3},{value:"3. Implementation Phases",id:"3-implementation-phases",level:3},{value:"Phase 1: Foundation Setup",id:"phase-1-foundation-setup",level:4},{value:"Phase 2: Perception Integration",id:"phase-2-perception-integration",level:4},{value:"Phase 3: Interaction Layer",id:"phase-3-interaction-layer",level:4},{value:"Phase 4: Integration and Validation",id:"phase-4-integration-and-validation",level:4},{value:"4. Core System Components",id:"4-core-system-components",level:3},{value:"Robot Middleware (ROS 2 Foundation)",id:"robot-middleware-ros-2-foundation",level:4},{value:"Perception System (Isaac Integration)",id:"perception-system-isaac-integration",level:4},{value:"Navigation System (Gazebo/Nav2 Integration)",id:"navigation-system-gazebonav2-integration",level:4},{value:"5. Integration Testing",id:"5-integration-testing",level:3},{value:"System Integration Tests",id:"system-integration-tests",level:4},{value:"Test Scenarios",id:"test-scenarios",level:4},{value:"6. Performance Optimization",id:"6-performance-optimization",level:3},{value:"Real-time Performance Considerations",id:"real-time-performance-considerations",level:4},{value:"Resource Management",id:"resource-management",level:4},{value:"Technology Stack",id:"technology-stack",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Quantitative Metrics",id:"quantitative-metrics",level:3},{value:"Qualitative Assessment",id:"qualitative-assessment",level:3},{value:"Project Deliverables",id:"project-deliverables",level:2},{value:"Timeline and Milestones",id:"timeline-and-milestones",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"autonomous-humanoid-robot-capstone-project",children:"Autonomous Humanoid Robot Capstone Project"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The capstone project integrates all four modules into a comprehensive autonomous humanoid robot system. Students will design, implement, and demonstrate a complete robotic system that combines ROS 2 fundamentals, simulation expertise, NVIDIA Isaac perception, and VLA-based human interaction. This project represents the culmination of the entire curriculum and demonstrates mastery of physical AI concepts."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By completing this capstone project, students will demonstrate:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integration of all four curriculum modules into a unified system"}),"\n",(0,s.jsx)(n.li,{children:"End-to-end implementation of a complex robotics application"}),"\n",(0,s.jsx)(n.li,{children:"Problem-solving skills applied to real-world robotics challenges"}),"\n",(0,s.jsx)(n.li,{children:"Professional-level documentation and system architecture design"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completion of all four modules (Modules 1-4)"}),"\n",(0,s.jsx)(n.li,{children:"Access to appropriate hardware or simulation environment"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of safety protocols for robot operation"}),"\n",(0,s.jsx)(n.li,{children:"Experience with debugging complex integrated systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-structure",children:"Project Structure"}),"\n",(0,s.jsx)(n.h3,{id:"1-system-architecture-design",children:"1. System Architecture Design"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High-level system design with component interfaces"}),"\n",(0,s.jsx)(n.li,{children:"Communication patterns between modules"}),"\n",(0,s.jsx)(n.li,{children:"Safety and error handling architecture"}),"\n",(0,s.jsx)(n.li,{children:"Performance requirements and constraints"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(n.p,{children:"The autonomous humanoid robot system integrates all curriculum components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Voice/Human   \u2502    \u2502  Perception &   \u2502    \u2502   Navigation &  \u2502\n\u2502   Interaction   \u2502\u2500\u2500\u2500\u25b6\u2502   Processing    \u2502\u2500\u2500\u2500\u25b6\u2502   Planning      \u2502\n\u2502 (Module 4: VLA) \u2502    \u2502 (Module 3: Isaac\u2502    \u2502 (Module 2: Gazebo\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502     ROS)        \u2502    \u2502     Nav2)       \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502                       \u2502\n                                \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   ROS 2 Core    \u2502\u25c0\u2500\u2500\u2500\u2502   State & Data   \u2502\u2500\u2500\u2500\u25b6\u2502  Actuation &    \u2502\n\u2502  (Module 1:     \u2502    \u2502   Management     \u2502    \u2502   Control       \u2502\n\u2502   Foundation)   \u2502    \u2502                  \u2502    \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u25bc\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502   Safety &       \u2502\n                        \u2502  Monitoring      \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-hardwaresoftware-requirements",children:"2. Hardware/Software Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Robot platform (physical or simulated)"}),"\n",(0,s.jsx)(n.li,{children:"Computing platform (NVIDIA Jetson, laptop with GPU, or cloud)"}),"\n",(0,s.jsx)(n.li,{children:"Sensors (LiDAR, cameras, IMU for physical; simulated equivalents)"}),"\n",(0,s.jsx)(n.li,{children:"Network connectivity for remote operations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-implementation-phases",children:"3. Implementation Phases"}),"\n",(0,s.jsx)(n.h4,{id:"phase-1-foundation-setup",children:"Phase 1: Foundation Setup"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Establish ROS 2 communication backbone"}),"\n",(0,s.jsx)(n.li,{children:"Set up robot description (URDF) and joint control"}),"\n",(0,s.jsx)(n.li,{children:"Configure basic navigation stack (costmaps, planners)"}),"\n",(0,s.jsx)(n.li,{children:"Implement basic safety protocols"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"phase-2-perception-integration",children:"Phase 2: Perception Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate Isaac perception pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Configure SLAM for localization and mapping"}),"\n",(0,s.jsx)(n.li,{children:"Implement object detection and tracking"}),"\n",(0,s.jsx)(n.li,{children:"Set up sensor fusion for state estimation"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"phase-3-interaction-layer",children:"Phase 3: Interaction Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement voice processing with Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Connect LLM for cognitive planning"}),"\n",(0,s.jsx)(n.li,{children:"Create natural language understanding pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Implement safety filters for voice commands"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"phase-4-integration-and-validation",children:"Phase 4: Integration and Validation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Connect all subsystems into unified system"}),"\n",(0,s.jsx)(n.li,{children:"Test end-to-end functionality"}),"\n",(0,s.jsx)(n.li,{children:"Validate safety protocols"}),"\n",(0,s.jsx)(n.li,{children:"Optimize performance and reliability"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-core-system-components",children:"4. Core System Components"}),"\n",(0,s.jsx)(n.h4,{id:"robot-middleware-ros-2-foundation",children:"Robot Middleware (ROS 2 Foundation)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, Pose\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nimport threading\nimport time\n\nclass HumanoidRobotCore(Node):\n    \"\"\"\n    Core node managing communication between all subsystems\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('humanoid_robot_core')\n\n        # Subscribers for all sensor inputs\n        self.odom_sub = self.create_subscription(Odometry, '/odom', self.odom_callback, 10)\n        self.scan_sub = self.create_subscription(LaserScan, '/scan', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, '/imu/data', self.imu_callback, 10)\n\n        # Subscribers for higher-level commands\n        self.voice_cmd_sub = self.create_subscription(String, '/voice_commands', self.voice_command_callback, 10)\n        self.nav_goal_sub = self.create_subscription(Pose, '/nav/goal', self.navigation_goal_callback, 10)\n\n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Robot state management\n        self.robot_state = {\n            'position': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'velocity': {'linear': 0.0, 'angular': 0.0},\n            'safety_status': 'nominal',\n            'battery_level': 100.0,\n            'active_tasks': []\n        }\n\n        # Safety timers and checks\n        self.last_command_time = time.time()\n        self.safety_timer = self.create_timer(0.1, self.safety_check)\n\n        self.get_logger().info('Humanoid Robot Core initialized')\n\n    def odom_callback(self, msg):\n        \"\"\"Update robot position from odometry\"\"\"\n        self.robot_state['position']['x'] = msg.pose.pose.position.x\n        self.robot_state['position']['y'] = msg.pose.pose.position.y\n\n        # Extract orientation from quaternion\n        import math\n        q = msg.pose.pose.orientation\n        siny_cosp = 2 * (q.w * q.z + q.x * q.y)\n        cosy_cosp = 1 - 2 * (q.y * q.y + q.z * q.z)\n        self.robot_state['position']['theta'] = math.atan2(siny_cosp, cosy_cosp)\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan for obstacle detection\"\"\"\n        # Check for nearby obstacles\n        min_distance = min([r for r in msg.ranges if r > 0 and not float('inf')], default=float('inf'))\n\n        if min_distance < 0.5:  # Less than 50cm to obstacle\n            self.robot_state['safety_status'] = 'obstacle_close'\n        else:\n            self.robot_state['safety_status'] = 'nominal'\n\n    def voice_command_callback(self, msg):\n        \"\"\"Process voice command and route to appropriate handler\"\"\"\n        self.last_command_time = time.time()\n\n        # Route command based on content\n        command = msg.data.lower()\n\n        if self.is_safe_to_execute(command):\n            self.route_command(command)\n        else:\n            self.log_safety_violation(command)\n\n    def is_safe_to_execute(self, command):\n        \"\"\"Check if command is safe to execute given current state\"\"\"\n        # Safety checks:\n        # 1. Battery level sufficient\n        if self.robot_state['battery_level'] < 10.0:\n            return command in ['return_to_base', 'charge', 'stop', 'halt']\n\n        # 2. No close obstacles for movement commands\n        if self.robot_state['safety_status'] == 'obstacle_close':\n            dangerous_movement = any(movement in command for movement in ['forward', 'move forward', 'go forward', 'approach'])\n            if dangerous_movement:\n                return False\n\n        # 3. Not in restricted area (would check against geofence)\n\n        return True\n\n    def route_command(self, command):\n        \"\"\"Route command to appropriate subsystem\"\"\"\n        if any(word in command for word in ['go to', 'navigate to', 'move to']):\n            self.handle_navigation_command(command)\n        elif any(word in command for word in ['pick up', 'grasp', 'get']):\n            self.handle_manipulation_command(command)\n        elif any(word in command for word in ['stop', 'halt', 'emergency']):\n            self.emergency_stop()\n        else:\n            # Default to cognitive planner\n            self.handle_cognitive_command(command)\n\n    def safety_check(self):\n        \"\"\"Regular safety checks\"\"\"\n        current_time = time.time()\n\n        # Check for command timeouts\n        if current_time - self.last_command_time > 30.0:  # 30 seconds without command\n            self.get_logger().warn('No commands received recently, stopping robot')\n            self.emergency_stop()\n\n    def emergency_stop(self):\n        \"\"\"Emergency stop - halt all movement\"\"\"\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n        self.get_logger().warn('EMERGENCY STOP ACTIVATED')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    robot_core = HumanoidRobotCore()\n\n    try:\n        rclpy.spin(robot_core)\n    except KeyboardInterrupt:\n        robot_core.get_logger().info('Shutting down Humanoid Robot Core')\n    finally:\n        robot_core.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h4,{id:"perception-system-isaac-integration",children:"Perception System (Isaac Integration)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport numpy as np\n\nclass HumanoidPerceptionNode(Node):\n    \"\"\"\n    Perception system handling Isaac-based processing\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('humanoid_perception')\n\n        # Subscribers for sensor data\n        self.rgb_sub = self.create_subscription(Image, '/camera/rgb/image_raw', self.rgb_callback, 10)\n        self.depth_sub = self.create_subscription(Image, '/camera/depth/image_raw', self.depth_callback, 10)\n        self.pointcloud_sub = self.create_subscription(PointCloud2, '/camera/depth/points', self.pointcloud_callback, 10)\n\n        # Publishers for processed data\n        self.object_detection_pub = self.create_publisher(String, '/perception/objects', 10)\n        self.scene_description_pub = self.create_publisher(String, '/perception/scene', 10)\n\n        # Initialize perception components\n        self.initialize_perception_models()\n\n        self.get_logger().info('Humanoid Perception System initialized')\n\n    def initialize_perception_models(self):\n        \"\"\"Initialize perception models (in practice, load Isaac ROS components)\"\"\"\n        # Placeholder for Isaac perception model initialization\n        self.object_detector_ready = True\n        self.scene_analyzer_ready = True\n\n    def rgb_callback(self, msg):\n        \"\"\"Process RGB image for object detection and scene understanding\"\"\"\n        if not self.object_detector_ready:\n            return\n\n        # In practice, this would run Isaac perception pipelines\n        # For this example, we'll simulate object detection\n        detected_objects = self.simulate_object_detection(msg)\n\n        if detected_objects:\n            obj_msg = String()\n            obj_msg.data = str(detected_objects)\n            self.object_detection_pub.publish(obj_msg)\n\n    def simulate_object_detection(self, image_msg):\n        \"\"\"Simulate object detection - in practice, use Isaac perception\"\"\"\n        # This would interface with Isaac ROS perception packages\n        # For simulation purposes, return mock detections\n        return [\n            {'class': 'person', 'confidence': 0.92, 'bbox': [100, 150, 200, 300]},\n            {'class': 'chair', 'confidence': 0.87, 'bbox': [300, 200, 450, 400]},\n            {'class': 'table', 'confidence': 0.95, 'bbox': [50, 300, 500, 450]}\n        ]\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = HumanoidPerceptionNode()\n\n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        perception_node.get_logger().info('Shutting down Perception System')\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h4,{id:"navigation-system-gazebonav2-integration",children:"Navigation System (Gazebo/Nav2 Integration)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, PoseStamped\nfrom nav_msgs.msg import Path\nfrom std_msgs.msg import String\nimport math\n\nclass HumanoidNavigationNode(Node):\n    """\n    Navigation system with Gazebo and Nav2 integration\n    """\n\n    def __init__(self):\n        super().__init__(\'humanoid_navigation\')\n\n        # Subscribers\n        self.goal_sub = self.create_subscription(Pose, \'/nav/goal\', self.navigation_goal_callback, 10)\n        self.cancel_sub = self.create_subscription(String, \'/nav/cancel\', self.cancel_navigation_callback, 10)\n\n        # Publishers\n        self.nav_goal_pub = self.create_publisher(PoseStamped, \'/move_base_simple/goal\', 10)\n        self.path_pub = self.create_publisher(Path, \'/nav/path\', 10)\n        self.status_pub = self.create_publisher(String, \'/nav/status\', 10)\n\n        # Navigation state\n        self.current_goal = None\n        self.navigation_active = False\n        self.path_history = []\n\n        self.get_logger().info(\'Humanoid Navigation System initialized\')\n\n    def navigation_goal_callback(self, msg):\n        """Handle navigation goal requests"""\n        goal_pose = PoseStamped()\n        goal_pose.header.stamp = self.get_clock().now().to_msg()\n        goal_pose.header.frame_id = \'map\'\n        goal_pose.pose = msg\n\n        self.nav_goal_pub.publish(goal_pose)\n        self.current_goal = goal_pose\n        self.navigation_active = True\n\n        status_msg = String()\n        status_msg.data = f\'navigating_to_x:{msg.position.x}_y:{msg.position.y}\'\n        self.status_pub.publish(status_msg)\n\n    def cancel_navigation_callback(self, msg):\n        """Cancel current navigation"""\n        if self.navigation_active:\n            self.cancel_current_goal()\n            self.navigation_active = False\n\n    def cancel_current_goal(self):\n        """Send cancel command to Nav2"""\n        # In practice, send action cancel request to Nav2\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nav_node = HumanoidNavigationNode()\n\n    try:\n        rclpy.spin(nav_node)\n    except KeyboardInterrupt:\n        nav_node.get_logger().info(\'Shutting down Navigation System\')\n    finally:\n        nav_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"5-integration-testing",children:"5. Integration Testing"}),"\n",(0,s.jsx)(n.h4,{id:"system-integration-tests",children:"System Integration Tests"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"End-to-End Navigation Test"}),": Verify the complete pipeline from voice command to successful navigation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception-Action Loop Test"}),": Validate that perceived objects influence navigation decisions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety System Test"}),": Confirm that safety protocols prevent dangerous behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Test"}),": Measure system response times and resource utilization"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"test-scenarios",children:"Test Scenarios"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Navigate to a specified location based on voice command"}),"\n",(0,s.jsx)(n.li,{children:"Avoid obstacles detected by perception system during navigation"}),"\n",(0,s.jsx)(n.li,{children:"Respond to emergency stop commands"}),"\n",(0,s.jsx)(n.li,{children:"Handle simultaneous perception and navigation requests"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"6-performance-optimization",children:"6. Performance Optimization"}),"\n",(0,s.jsx)(n.h4,{id:"real-time-performance-considerations",children:"Real-time Performance Considerations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Minimize latency in voice processing pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Optimize perception algorithms for real-time operation"}),"\n",(0,s.jsx)(n.li,{children:"Ensure navigation planning runs within timing constraints"}),"\n",(0,s.jsx)(n.li,{children:"Implement efficient data buffering and processing"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"resource-management",children:"Resource Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor CPU, GPU, and memory utilization"}),"\n",(0,s.jsx)(n.li,{children:"Implement dynamic resource allocation based on task priority"}),"\n",(0,s.jsx)(n.li,{children:"Optimize network usage for distributed components"}),"\n",(0,s.jsx)(n.li,{children:"Implement graceful degradation when resources are constrained"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"technology-stack",children:"Technology Stack"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Core Framework"}),": ROS 2 Humble Hawksbill"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simulation"}),": Gazebo Garden with Isaac integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": Isaac ROS packages for vision and sensing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation"}),": Nav2 with custom controllers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interaction"}),": Whisper for voice, open-source LLMs for planning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware"}),": NVIDIA Jetson platform or GPU-enabled workstation"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,s.jsx)(n.p,{children:"Students will demonstrate successful completion by:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System Integration"}),": All four modules working together as a unified system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Autonomous Operation"}),": Robot responds appropriately to voice commands without human intervention"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Compliance"}),": All safety protocols function correctly preventing dangerous behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance"}),": System operates within specified timing and resource constraints"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Documentation"}),": Complete system documentation including design decisions, implementation details, and lessons learned"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"quantitative-metrics",children:"Quantitative Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Completion Rate"}),": 90% of commanded tasks executed successfully"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation Success Rate"}),": 95% of navigation goals reached without collisions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Command Accuracy"}),": 90% of voice commands correctly interpreted and executed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System Uptime"}),": 99% availability during testing period"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Time"}),": Voice commands processed and acted upon within 5 seconds"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"qualitative-assessment",children:"Qualitative Assessment"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System Design"}),": Elegant architecture with clear component separation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Problem-Solving"}),": Creative solutions to integration challenges"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Documentation"}),": Clear, comprehensive, and useful documentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Awareness"}),": Thoughtful implementation of safety protocols"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": Graceful handling of errors and edge cases"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-deliverables",children:"Project Deliverables"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Source Code"}),": Complete, well-documented implementation of all system components"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Technical Report"}),": Comprehensive documentation of design decisions, implementation approach, and results"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Demonstration"}),": Video showing system performing various tasks successfully"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Presentation"}),": Technical presentation explaining system architecture and key innovations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Maintenance Guide"}),": Instructions for system setup, operation, and troubleshooting"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"timeline-and-milestones",children:"Timeline and Milestones"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Week 1-2"}),": System design and architecture planning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Week 3-4"}),": Core infrastructure and ROS 2 integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Week 5-6"}),": Perception and navigation system integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Week 7-8"}),": Voice interaction and cognitive planning integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Week 9-10"}),": System integration, testing, and optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Week 11-12"}),": Documentation, presentation preparation, and final demonstration"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This capstone project represents the culmination of the entire curriculum, challenging students to synthesize all learned concepts into a working autonomous humanoid robot system. Success requires deep understanding of all four modules and the ability to integrate complex systems while maintaining safety and performance standards."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);