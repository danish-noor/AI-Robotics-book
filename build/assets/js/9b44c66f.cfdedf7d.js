"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[213],{4979(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-3/index","title":"NVIDIA Isaac & Perception","description":"Overview","source":"@site/docs/module-3/index.md","sourceDirName":"module-3","slug":"/module-3/","permalink":"/AI-Robotics-book/docs/module-3/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1768307371000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Simulation & Digital Twins","permalink":"/AI-Robotics-book/docs/module-2/"},"next":{"title":"Vision-Language-Action (VLA) & Human-Robot Interaction","permalink":"/AI-Robotics-book/docs/module-4/"}}');var o=i(4848),s=i(8453);const t={},r="NVIDIA Isaac & Perception",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"1. NVIDIA Isaac Ecosystem",id:"1-nvidia-isaac-ecosystem",level:3},{value:"Isaac Sim Setup and Configuration",id:"isaac-sim-setup-and-configuration",level:4},{value:"2. Isaac Sim Advanced Simulation",id:"2-isaac-sim-advanced-simulation",level:3},{value:"Creating Photorealistic Environments in Isaac Sim",id:"creating-photorealistic-environments-in-isaac-sim",level:4},{value:"3. Isaac ROS Perception Pipeline",id:"3-isaac-ros-perception-pipeline",level:3},{value:"Isaac ROS Perception Pipeline Example",id:"isaac-ros-perception-pipeline-example",level:4},{value:"4. Visual SLAM (VSLAM)",id:"4-visual-slam-vslam",level:3},{value:"VSLAM Implementation with Isaac",id:"vslam-implementation-with-isaac",level:4},{value:"5. Nav2 Navigation System",id:"5-nav2-navigation-system",level:3},{value:"Nav2 Configuration for Isaac Integration",id:"nav2-configuration-for-isaac-integration",level:4},{value:"6. Perception-Action Integration",id:"6-perception-action-integration",level:3},{value:"Perception-Action Integration Example",id:"perception-action-integration-example",level:4},{value:"Technology Stack",id:"technology-stack",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Exercise 1: Isaac Sim Environment Setup",id:"exercise-1-isaac-sim-environment-setup",level:3},{value:"Exercise 2: VSLAM Implementation",id:"exercise-2-vslam-implementation",level:3},{value:"Exercise 3: Nav2 Configuration with Isaac",id:"exercise-3-nav2-configuration-with-isaac",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2}];function _(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"nvidia-isaac--perception",children:"NVIDIA Isaac & Perception"})}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"NVIDIA Isaac represents the industrial-grade ecosystem for robotics perception, computer vision, and navigation. This module explores Isaac Sim for advanced simulation and Isaac ROS for perception processing, with emphasis on Visual Simultaneous Localization and Mapping (VSLAM) and Nav2 path planning integration."}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Utilize NVIDIA Isaac Sim for advanced robotics simulation"}),"\n",(0,o.jsx)(e.li,{children:"Implement Isaac ROS packages for perception processing"}),"\n",(0,o.jsx)(e.li,{children:"Execute VSLAM algorithms for localization and mapping"}),"\n",(0,o.jsx)(e.li,{children:"Configure Nav2 for autonomous navigation in complex environments"}),"\n",(0,o.jsx)(e.li,{children:"Integrate perception outputs with navigation systems"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Completion of Module 1 (ROS 2 fundamentals)"}),"\n",(0,o.jsx)(e.li,{children:"Completion of Module 2 (Simulation & Digital Twins)"}),"\n",(0,o.jsx)(e.li,{children:"Understanding of computer vision concepts"}),"\n",(0,o.jsx)(e.li,{children:"Familiarity with CUDA and GPU computing concepts"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,o.jsx)(e.h3,{id:"1-nvidia-isaac-ecosystem",children:"1. NVIDIA Isaac Ecosystem"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Isaac Sim architecture and capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Isaac ROS packages overview"}),"\n",(0,o.jsx)(e.li,{children:"Hardware requirements and setup"}),"\n",(0,o.jsx)(e.li,{children:"Integration with ROS 2 ecosystem"}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"isaac-sim-setup-and-configuration",children:"Isaac Sim Setup and Configuration"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Prerequisites:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"NVIDIA GPU with CUDA support (RTX 4090, RTX 6000 Ada, or A6000 recommended)"}),"\n",(0,o.jsx)(e.li,{children:"NVIDIA Driver 535 or later"}),"\n",(0,o.jsx)(e.li,{children:"CUDA 12.1 or later"}),"\n",(0,o.jsx)(e.li,{children:"Isaac Sim requires substantial resources: 16GB+ RAM, 50GB+ disk space"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Installation:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Download Isaac Sim from NVIDIA Developer portal\n# Extract to desired location\ncd /path/to/isaac_sim\n./isaac-sim-launch.sh\n"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"ROS 2 Integration:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Install Isaac ROS packages for Humble\nsudo apt install ros-humble-isaac-ros-* ros-humble-nitros-*\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-isaac-sim-advanced-simulation",children:"2. Isaac Sim Advanced Simulation"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Creating photorealistic environments"}),"\n",(0,o.jsx)(e.li,{children:"Sensor simulation with Isaac Sim"}),"\n",(0,o.jsx)(e.li,{children:"Domain randomization techniques"}),"\n",(0,o.jsx)(e.li,{children:"Synthetic data generation for training"}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"creating-photorealistic-environments-in-isaac-sim",children:"Creating Photorealistic Environments in Isaac Sim"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Example USD Scene Configuration:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import omni\nfrom pxr import UsdGeom, Gf, Sdf\n\ndef create_photorealistic_scene(stage):\n    """\n    Create a photorealistic scene with lighting, materials, and objects\n    """\n    # Create a prim for the scene\n    scene_prim = stage.DefinePrim("/World", "Xform")\n\n    # Add dome light for realistic illumination\n    dome_light = stage.DefinePrim("/World/DomeLight", "DomeLight")\n    dome_light.GetAttribute("inputs:intensity").Set(500)\n    dome_light.GetAttribute("inputs:color").Set(Gf.Vec3f(0.8, 0.8, 1.0))\n\n    # Add environment\n    env_prim = stage.DefinePrim("/World/Environment", "Xform")\n    env_prim.GetAttribute("xformOp:translate").Set(Gf.Vec3d(0, 0, 0))\n\n    return scene_prim\n'})}),"\n",(0,o.jsx)(e.h3,{id:"3-isaac-ros-perception-pipeline",children:"3. Isaac ROS Perception Pipeline"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Image acquisition and preprocessing"}),"\n",(0,o.jsx)(e.li,{children:"Object detection and tracking"}),"\n",(0,o.jsx)(e.li,{children:"3D perception and reconstruction"}),"\n",(0,o.jsx)(e.li,{children:"Sensor calibration and fusion"}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"isaac-ros-perception-pipeline-example",children:"Isaac ROS Perception Pipeline Example"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"ROS 2 Node for Image Processing:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacROSImageProcessor(Node):\n\n    def __init__(self):\n        super().__init__('isaac_ros_image_processor')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Create publishers\n        self.processed_pub = self.create_publisher(\n            Image,\n            '/camera/processed/image',\n            10\n        )\n\n        self.get_logger().info('Isaac ROS Image Processor initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Apply Isaac ROS compatible processing\n            processed_image = self.apply_perception_pipeline(cv_image)\n\n            # Convert back to ROS Image\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding='bgr8')\n            processed_msg.header = msg.header\n\n            # Publish processed image\n            self.processed_pub.publish(processed_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\n    def apply_perception_pipeline(self, image):\n        \"\"\"Apply perception pipeline to image\"\"\"\n        # Placeholder for actual Isaac ROS perception pipeline\n        # In real implementation, this would use Isaac ROS packages\n        return image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = IsaacROSImageProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"4-visual-slam-vslam",children:"4. Visual SLAM (VSLAM)"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"SLAM fundamentals and approaches"}),"\n",(0,o.jsx)(e.li,{children:"Visual-inertial odometry"}),"\n",(0,o.jsx)(e.li,{children:"Loop closure and map optimization"}),"\n",(0,o.jsx)(e.li,{children:"Real-time mapping in complex environments"}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"vslam-implementation-with-isaac",children:"VSLAM Implementation with Isaac"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Example VSLAM Node:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nimport numpy as np\n\nclass IsaacVSLAMNode(Node):\n\n    def __init__(self):\n        super().__init__(\'isaac_vslam_node\')\n\n        # Initialize VSLAM components\n        self.initialize_vslam()\n\n        # Create subscribers for camera and IMU data\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/stereo_left/image_rect\',\n            self.stereo_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Create publishers for pose and map\n        self.pose_pub = self.create_publisher(PoseStamped, \'/vslam/pose\', 10)\n        self.odom_pub = self.create_publisher(Odometry, \'/vslam/odometry\', 10)\n\n        self.get_logger().info(\'Isaac VSLAM node initialized\')\n\n    def initialize_vslam(self):\n        """Initialize VSLAM components"""\n        # Placeholder for actual VSLAM initialization\n        # In real implementation, this would initialize Isaac\'s VSLAM system\n        self.camera_matrix = np.eye(3)\n        self.distortion_coeffs = np.zeros(5)\n        self.current_pose = np.eye(4)\n\n    def stereo_callback(self, msg):\n        """Process stereo camera images for VSLAM"""\n        # Process stereo images to extract features and estimate motion\n        pass\n\n    def imu_callback(self, msg):\n        """Process IMU data for sensor fusion"""\n        # Process IMU data to improve pose estimation\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vslam_node = IsaacVSLAMNode()\n\n    try:\n        rclpy.spin(vslam_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vslam_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"5-nav2-navigation-system",children:"5. Nav2 Navigation System"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Costmap configuration and layers"}),"\n",(0,o.jsx)(e.li,{children:"Global and local planners"}),"\n",(0,o.jsx)(e.li,{children:"Controller configuration"}),"\n",(0,o.jsx)(e.li,{children:"Behavior trees for navigation"}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"nav2-configuration-for-isaac-integration",children:"Nav2 Configuration for Isaac Integration"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"nav2_params.yaml:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:'amcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_footprint"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    likelihood_max_dist: 2.0\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n    scan_topic: scan\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    default_nav_through_poses_bt_xml: "nav2_bt_xml_v0_forward_compatible.xml"\n    default_nav_to_pose_bt_xml: "nav2_bt_xml_v0_forward_compatible.xml"\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_compute_path_through_poses_action_bt_node\n    - nav2_smooth_path_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n    - nav2_assisted_teleop_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_drive_on_heading_bt_node\n    - nav2_clear_costmap_service_bt_node\n    - nav2_is_stuck_condition_bt_node\n    - nav2_goal_reached_condition_bt_node\n    - nav2_goal_updated_condition_bt_node\n    - nav2_initial_pose_received_condition_bt_node\n    - nav2_reinitialize_global_localization_service_bt_node\n    - nav2_rate_controller_bt_node\n    - nav2_distance_controller_bt_node\n    - nav2_speed_controller_bt_node\n    - nav2_truncate_path_action_bt_node\n    - nav2_truncate_path_local_action_bt_node\n    - nav2_goal_updater_node_bt_node\n    - nav2_recovery_node_bt_node\n    - nav2_pipeline_sequence_bt_node\n    - nav2_round_robin_node_bt_node\n    - nav2_transform_available_condition_bt_node\n    - nav2_time_expired_condition_bt_node\n    - nav2_path_expiring_timer_condition\n    - nav2_distance_traveled_condition_bt_node\n    - nav2_single_trigger_bt_node\n    - nav2_is_battery_low_condition_bt_node\n    - nav2_navigate_through_poses_action_bt_node\n    - nav2_navigate_to_pose_action_bt_node\n    - nav2_remove_passed_goals_action_bt_node\n    - nav2_planner_selector_bt_node\n    - nav2_controller_selector_bt_node\n    - nav2_goal_checker_selector_bt_node\n    - nav2_controller_cancel_bt_node\n    - nav2_path_longer_on_approach_bt_node\n    - nav2_wait_cancel_bt_node\n    - nav2_spin_cancel_bt_node\n    - nav2_back_up_cancel_bt_node\n    - nav2_assisted_teleop_cancel_bt_node\n    - nav2_drive_on_heading_cancel_bt_node\n    - nav2_is_battery_charging_condition_bt_node\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["FollowPath"]\n\n    # Progress checker parameters\n    progress_checker:\n      plugin: "nav2_controller::SimpleProgressChecker"\n      required_movement_radius: 0.5\n      movement_time_allowance: 10.0\n\n    # Goal checker parameters\n    goal_checker:\n      plugin: "nav2_controller::SimpleGoalChecker"\n      xy_goal_tolerance: 0.25\n      yaw_goal_tolerance: 0.25\n      stateful: True\n\n    # DWB parameters\n    FollowPath:\n      plugin: "nav2_rotation_shim_controller::RotationShimController"\n      primary_controller: "dwb_core::DWBLocalPlanner"\n      smooth_rotate: True\n\n      dwb_core:\n        plugin: "dwb_core::DWBLocalPlanner"\n        debug_trajectory_details: True\n        min_vel_x: 0.0\n        min_vel_y: 0.0\n        max_vel_x: 0.5\n        max_vel_y: 0.0\n        max_vel_theta: 1.0\n        min_speed_xy: 0.0\n        max_speed_xy: 0.5\n        min_speed_theta: 0.0\n        acc_lim_x: 2.5\n        acc_lim_y: 0.0\n        acc_lim_theta: 3.2\n        decel_lim_x: -2.5\n        decel_lim_y: 0.0\n        decel_lim_theta: -3.2\n        vx_samples: 20\n        vy_samples: 5\n        vtheta_samples: 20\n        sim_time: 1.7\n        linear_granularity: 0.05\n        angular_granularity: 0.025\n        transform_tolerance: 0.2\n        xy_goal_tolerance: 0.25\n        trans_stopped_velocity: 0.25\n        short_circuit_trajectory_evaluation: True\n        stateful: True\n        critics: ["RotateToGoal", "Oscillation", "BaseObstacle", "GoalAlign", "PathAlign", "PathDist", "GoalDist"]\n        BaseObstacle.scale: 0.02\n        PathAlign.scale: 32.0\n        PathAlign.forward_point_distance: 0.1\n        GoalAlign.scale: 24.0\n        GoalAlign.forward_point_distance: 0.1\n        PathDist.scale: 32.0\n        GoalDist.scale: 24.0\n        RotateToGoal.scale: 32.0\n        RotateToGoal.slowing_factor: 5.0\n        RotateToGoal.lookahead_time: -1.0\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      global_frame: odom\n      robot_base_frame: base_link\n      use_sim_time: True\n      rolling_window: true\n      width: 3\n      height: 3\n      resolution: 0.05\n      robot_radius: 0.22\n      plugins: ["voxel_layer", "inflation_layer"]\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n      voxel_layer:\n        plugin: "nav2_costmap_2d::VoxelLayer"\n        enabled: True\n        publish_voxel_map: False\n        origin_z: 0.0\n        z_resolution: 0.2\n        z_voxels: 10\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n'})}),"\n",(0,o.jsx)(e.h3,{id:"6-perception-action-integration",children:"6. Perception-Action Integration"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Closing the perception-action loop"}),"\n",(0,o.jsx)(e.li,{children:"Real-time navigation with perception feedback"}),"\n",(0,o.jsx)(e.li,{children:"Dynamic obstacle avoidance"}),"\n",(0,o.jsx)(e.li,{children:"Multi-sensor fusion for navigation"}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"perception-action-integration-example",children:"Perception-Action Integration Example"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Integrated Navigation Node:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nimport numpy as np\n\nclass IntegratedNavigationNode(Node):\n\n    def __init__(self):\n        super().__init__(\'integrated_navigation\')\n\n        # Initialize navigation and perception components\n        self.setup_navigation_system()\n        self.setup_perception_system()\n\n        # Create subscribers\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.lidar_callback,\n            10\n        )\n\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            \'/odom\',\n            self.odom_callback,\n            10\n        )\n\n        # Create publisher for velocity commands\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Timer for navigation loop\n        self.nav_timer = self.create_timer(0.1, self.navigation_loop)\n\n        self.get_logger().info(\'Integrated Navigation Node initialized\')\n\n    def setup_navigation_system(self):\n        """Setup navigation system components"""\n        # Initialize path planner, local planner, etc.\n        pass\n\n    def setup_perception_system(self):\n        """Setup perception system components"""\n        # Initialize object detection, tracking, etc.\n        pass\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data for obstacle detection"""\n        # Analyze LiDAR data for obstacles\n        ranges = np.array(msg.ranges)\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        # Detect obstacles and update costmap\n        self.update_obstacle_information(valid_ranges)\n\n    def navigation_loop(self):\n        """Main navigation control loop"""\n        # Plan path to goal\n        # Sense environment\n        # Adjust path based on perception\n        # Execute control commands\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nav_node = IntegratedNavigationNode()\n\n    try:\n        rclpy.spin(nav_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        nav_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"technology-stack",children:"Technology Stack"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Simulation"}),": NVIDIA Isaac Sim (advanced robotics simulation)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Perception"}),": Isaac ROS packages (computer vision and perception)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation"}),": Nav2 (Navigation Stack 2) with Isaac integration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"SLAM"}),": Isaac ROS VSLAM components"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Hardware"}),": NVIDIA Jetson or GPU-enabled systems (RTX 4090, A6000, etc.)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS 2 Version"}),": Compatible with Humble Hawksbill"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"exercise-1-isaac-sim-environment-setup",children:"Exercise 1: Isaac Sim Environment Setup"}),"\n",(0,o.jsx)(e.p,{children:"Set up Isaac Sim with a complex 3D environment:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Install Isaac Sim with proper GPU drivers and CUDA support"}),"\n",(0,o.jsx)(e.li,{children:"Create a photorealistic environment with obstacles"}),"\n",(0,o.jsx)(e.li,{children:"Configure sensor simulation (LiDAR, cameras, IMU)"}),"\n",(0,o.jsx)(e.li,{children:"Verify that Isaac Sim integrates properly with ROS 2"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-2-vslam-implementation",children:"Exercise 2: VSLAM Implementation"}),"\n",(0,o.jsx)(e.p,{children:"Implement VSLAM for localization in the simulated environment:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Configure Isaac's VSLAM pipeline for your robot"}),"\n",(0,o.jsx)(e.li,{children:"Test localization accuracy in different lighting conditions"}),"\n",(0,o.jsx)(e.li,{children:"Validate mapping quality in complex environments"}),"\n",(0,o.jsx)(e.li,{children:"Compare SLAM performance with ground truth from simulation"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-3-nav2-configuration-with-isaac",children:"Exercise 3: Nav2 Configuration with Isaac"}),"\n",(0,o.jsx)(e.p,{children:"Configure Nav2 for autonomous navigation with perception feedback:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Set up costmap layers with Isaac sensor data"}),"\n",(0,o.jsx)(e.li,{children:"Configure global and local planners for your robot"}),"\n",(0,o.jsx)(e.li,{children:"Test navigation in dynamic environments with moving obstacles"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate navigation success rates and path efficiency"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,o.jsx)(e.p,{children:"Students will demonstrate proficiency by:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Setting up Isaac Sim with a complex 3D environment"}),"\n",(0,o.jsx)(e.li,{children:"Implementing VSLAM for localization and achieving accurate mapping"}),"\n",(0,o.jsx)(e.li,{children:"Configuring Nav2 for autonomous navigation with perception feedback"}),"\n",(0,o.jsx)(e.li,{children:"Successfully navigating to goals while avoiding dynamic obstacles"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"This module provides exposure to industry-standard tools used in professional robotics applications, bridging academic learning with real-world deployment scenarios."})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(_,{...n})}):_(n)}},8453(n,e,i){i.d(e,{R:()=>t,x:()=>r});var a=i(6540);const o={},s=a.createContext(o);function t(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:t(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);