"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[267],{454(n,e,i){i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module-4/index","title":"Vision-Language-Action (VLA) & Human-Robot Interaction","description":"Overview","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/AI-Robotics-book/docs/module-4/","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1768307371000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac & Perception","permalink":"/AI-Robotics-book/docs/module-3/"}}');var t=i(4848),a=i(8453);const r={},s="Vision-Language-Action (VLA) & Human-Robot Interaction",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"1. Voice Processing with Whisper",id:"1-voice-processing-with-whisper",level:3},{value:"Whisper Integration with ROS 2",id:"whisper-integration-with-ros-2",level:4},{value:"2. Large Language Models for Cognitive Planning",id:"2-large-language-models-for-cognitive-planning",level:3},{value:"LLM Integration for Robotic Planning",id:"llm-integration-for-robotic-planning",level:4},{value:"3. Vision-Language Integration",id:"3-vision-language-integration",level:3},{value:"Vision-Language Integration Example",id:"vision-language-integration-example",level:4},{value:"4. Human-Robot Interaction Design",id:"4-human-robot-interaction-design",level:3},{value:"Safe Voice-Controlled Robot Interface",id:"safe-voice-controlled-robot-interface",level:4},{value:"5. VLA Model Integration",id:"5-vla-model-integration",level:3},{value:"Complete VLA Integration Example",id:"complete-vla-integration-example",level:4},{value:"Technology Stack",id:"technology-stack",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Exercise 1: Voice Command Recognition",id:"exercise-1-voice-command-recognition",level:3},{value:"Exercise 2: Cognitive Planning Implementation",id:"exercise-2-cognitive-planning-implementation",level:3},{value:"Exercise 3: Vision-Language Grounding",id:"exercise-3-vision-language-grounding",level:3},{value:"Exercise 4: Safe Human-Robot Interaction",id:"exercise-4-safe-human-robot-interaction",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"vision-language-action-vla--human-robot-interaction",children:"Vision-Language-Action (VLA) & Human-Robot Interaction"})}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"Vision-Language-Action (VLA) models represent the frontier of human-robot interaction, enabling robots to understand natural language commands and execute complex tasks by grounding language in physical actions. This module focuses on integrating Whisper for voice processing with large language models for cognitive planning, creating intelligent systems that can interpret and act on human instructions."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate Whisper for voice command recognition and processing"}),"\n",(0,t.jsx)(e.li,{children:"Connect large language models to robotic control systems for cognitive planning"}),"\n",(0,t.jsx)(e.li,{children:"Implement cognitive architectures for task decomposition"}),"\n",(0,t.jsx)(e.li,{children:"Create natural language interfaces for robot control"}),"\n",(0,t.jsx)(e.li,{children:"Design safe and effective human-robot interaction patterns"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Completion of Module 1 (ROS 2 fundamentals)"}),"\n",(0,t.jsx)(e.li,{children:"Completion of Module 2 (Simulation & Digital Twins)"}),"\n",(0,t.jsx)(e.li,{children:"Completion of Module 3 (NVIDIA Isaac & Perception)"}),"\n",(0,t.jsx)(e.li,{children:"Understanding of machine learning concepts"}),"\n",(0,t.jsx)(e.li,{children:"Familiarity with Python for AI integration"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsx)(e.h3,{id:"1-voice-processing-with-whisper",children:"1. Voice Processing with Whisper"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Whisper model integration and setup"}),"\n",(0,t.jsx)(e.li,{children:"Real-time speech recognition in robotics contexts"}),"\n",(0,t.jsx)(e.li,{children:"Voice command parsing and validation"}),"\n",(0,t.jsx)(e.li,{children:"Audio preprocessing and noise filtering"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"whisper-integration-with-ros-2",children:"Whisper Integration with ROS 2"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Setting up Whisper for Robotics:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Install Whisper dependencies\npip install openai-whisper\npip install sounddevice  # For audio input\npip install pyaudio      # Alternative audio input\n"})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"ROS 2 Node for Voice Processing:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nfrom std_msgs.msg import String\nimport threading\nimport queue\n\nclass WhisperVoiceNode(Node):\n\n    def __init__(self):\n        super().__init__(\'whisper_voice_node\')\n\n        # Initialize Whisper model\n        self.model = whisper.load_model("base")  # Choose: tiny, base, small, medium, large\n\n        # Create publisher for recognized commands\n        self.command_pub = self.create_publisher(String, \'/voice_commands\', 10)\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.audio_queue = queue.Queue()\n\n        # Start audio recording thread\n        self.recording_thread = threading.Thread(target=self.record_audio, daemon=True)\n        self.recording_thread.start()\n\n        # Timer for processing audio chunks\n        self.process_timer = self.create_timer(2.0, self.process_audio_chunk)\n\n        self.get_logger().info(\'Whisper Voice Node initialized\')\n\n    def record_audio(self):\n        """Record audio continuously and put chunks in queue"""\n        def audio_callback(indata, frames, time, status):\n            if status:\n                self.get_logger().warning(f\'Audio callback status: {status}\')\n            # Copy audio data to avoid buffer issues\n            self.audio_queue.put(indata.copy())\n\n        with sd.InputStream(callback=audio_callback,\n                          channels=1,\n                          samplerate=self.sample_rate,\n                          dtype=np.float32):\n            while rclpy.ok():\n                sd.sleep(100)\n\n    def process_audio_chunk(self):\n        """Process accumulated audio and run through Whisper"""\n        if not self.audio_queue.empty():\n            # Collect audio data from queue\n            audio_data = []\n            while not self.audio_queue.empty():\n                chunk = self.audio_queue.get()\n                audio_data.append(chunk.flatten())\n\n            if audio_data:\n                # Concatenate all audio chunks\n                full_audio = np.concatenate(audio_data)\n\n                # Convert to float32 and normalize if needed\n                if full_audio.dtype != np.float32:\n                    full_audio = full_audio.astype(np.float32)\n\n                # Run through Whisper\n                result = self.model.transcribe(full_audio)\n                text = result[\'text\'].strip()\n\n                if text:  # Only publish if there\'s actual text\n                    cmd_msg = String()\n                    cmd_msg.data = text\n                    self.command_pub.publish(cmd_msg)\n                    self.get_logger().info(f\'Recognized: "{text}"\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_node = WhisperVoiceNode()\n\n    try:\n        rclpy.spin(voice_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"2-large-language-models-for-cognitive-planning",children:"2. Large Language Models for Cognitive Planning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Open-source LLM integration (practical approach)"}),"\n",(0,t.jsx)(e.li,{children:"Prompt engineering for robotic tasks"}),"\n",(0,t.jsx)(e.li,{children:"Context management for ongoing interactions"}),"\n",(0,t.jsx)(e.li,{children:"Safety and filtering for robot commands"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"llm-integration-for-robotic-planning",children:"LLM Integration for Robotic Planning"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Cognitive Planning Node:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nimport json\nimport re\n\nclass CognitivePlannerNode(Node):\n\n    def __init__(self):\n        super().__init__('cognitive_planner')\n\n        # Create subscribers\n        self.voice_sub = self.create_subscription(\n            String,\n            '/voice_commands',\n            self.voice_command_callback,\n            10\n        )\n\n        # Create publishers for robot actions\n        self.move_pub = self.create_publisher(Pose, '/move_base_simple/goal', 10)\n        self.action_pub = self.create_publisher(String, '/robot_actions', 10)\n\n        # Store robot state and context\n        self.robot_state = {\n            'position': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'battery_level': 100.0,\n            'current_task': None\n        }\n\n        self.get_logger().info('Cognitive Planner Node initialized')\n\n    def voice_command_callback(self, msg):\n        \"\"\"Process voice command and generate robot actions\"\"\"\n        command_text = msg.data.lower()\n\n        # Generate robot actions based on command\n        actions = self.generate_actions_from_command(command_text)\n\n        if actions:\n            for action in actions:\n                self.execute_action(action)\n\n    def generate_actions_from_command(self, command):\n        \"\"\"Convert natural language command to robot actions using LLM-style logic\"\"\"\n        # This is a simplified example - in practice, you'd use an actual LLM\n        actions = []\n\n        # Parse command for navigation intents\n        if any(word in command for word in ['go to', 'navigate to', 'move to', 'travel to']):\n            # Extract location using regex or NLP\n            location_match = re.search(r'(kitchen|living room|bedroom|office|bathroom)', command)\n            if location_match:\n                location = location_match.group(1)\n                goal_pose = self.get_location_pose(location)\n                if goal_pose:\n                    actions.append({\n                        'type': 'navigation',\n                        'target': location,\n                        'pose': goal_pose\n                    })\n\n        # Parse command for manipulation intents\n        elif any(word in command for word in ['pick up', 'grab', 'take', 'get']):\n            # Extract object\n            object_match = re.search(r'(ball|cup|book|phone|toy)', command)\n            if object_match:\n                obj = object_match.group(1)\n                actions.append({\n                    'type': 'manipulation',\n                    'action': 'pick_up',\n                    'object': obj\n                })\n\n        # Parse command for basic movements\n        elif any(word in command for word in ['forward', 'backward', 'left', 'right', 'turn']):\n            if 'forward' in command:\n                actions.append({'type': 'movement', 'direction': 'forward', 'distance': 1.0})\n            elif 'backward' in command:\n                actions.append({'type': 'movement', 'direction': 'backward', 'distance': 1.0})\n            elif 'left' in command or 'turn left' in command:\n                actions.append({'type': 'rotation', 'angle': 90.0})\n            elif 'right' in command or 'turn right' in command:\n                actions.append({'type': 'rotation', 'angle': -90.0})\n\n        return actions\n\n    def get_location_pose(self, location_name):\n        \"\"\"Get predefined poses for common locations\"\"\"\n        locations = {\n            'kitchen': {'x': 2.0, 'y': 1.0, 'theta': 0.0},\n            'living room': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'bedroom': {'x': -1.0, 'y': 2.0, 'theta': 1.57},\n            'office': {'x': 1.5, 'y': -1.0, 'theta': -1.57},\n            'bathroom': {'x': -0.5, 'y': -0.5, 'theta': 3.14}\n        }\n\n        loc_data = locations.get(location_name.lower())\n        if loc_data:\n            pose = Pose()\n            pose.position.x = loc_data['x']\n            pose.position.y = loc_data['y']\n            pose.position.z = 0.0\n            # Simple theta to quaternion conversion (assuming only z-rotation)\n            from math import sin, cos\n            half_theta = loc_data['theta'] / 2.0\n            pose.orientation.z = sin(half_theta)\n            pose.orientation.w = cos(half_theta)\n            return pose\n        return None\n\n    def execute_action(self, action):\n        \"\"\"Execute the planned action\"\"\"\n        if action['type'] == 'navigation':\n            self.move_pub.publish(action['pose'])\n            self.get_logger().info(f'Navigating to {action[\"target\"]}')\n        elif action['type'] == 'manipulation':\n            cmd_msg = String()\n            cmd_msg.data = f\"{action['action']}_{action['object']}\"\n            self.action_pub.publish(cmd_msg)\n            self.get_logger().info(f'Attempting to {action[\"action\"]} {action[\"object\"]}')\n        elif action['type'] == 'movement':\n            # Publish movement command\n            cmd_msg = String()\n            cmd_msg.data = f\"move_{action['direction']}_{action['distance']}\"\n            self.action_pub.publish(cmd_msg)\n            self.get_logger().info(f'Moving {action[\"direction\"]} by {action[\"distance\"]}m')\n        elif action['type'] == 'rotation':\n            # Publish rotation command\n            cmd_msg = String()\n            cmd_msg.data = f\"rotate_{action['angle']}\"\n            self.action_pub.publish(cmd_msg)\n            self.get_logger().info(f'Rotating by {action[\"angle\"]} degrees')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner_node = CognitivePlannerNode()\n\n    try:\n        rclpy.spin(planner_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        planner_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"3-vision-language-integration",children:"3. Vision-Language Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Multimodal perception systems"}),"\n",(0,t.jsx)(e.li,{children:"Grounding language commands in visual context"}),"\n",(0,t.jsx)(e.li,{children:"Object detection and recognition for action planning"}),"\n",(0,t.jsx)(e.li,{children:"Scene understanding for spatial reasoning"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"vision-language-integration-example",children:"Vision-Language Integration Example"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Multimodal Perception Node:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass VisionLanguageNode(Node):\n\n    def __init__(self):\n        super().__init__(\'vision_language_node\')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to camera feed\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Subscribe to voice commands\n        self.voice_sub = self.create_subscription(\n            String,\n            \'/voice_commands\',\n            self.voice_callback,\n            10\n        )\n\n        # Publisher for grounded actions\n        self.action_pub = self.create_publisher(String, \'/grounded_actions\', 10)\n\n        # Store current image for processing with voice commands\n        self.current_image = None\n        self.pending_command = None\n\n        self.get_logger().info(\'Vision-Language Integration Node initialized\')\n\n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        try:\n            # Convert ROS Image to OpenCV\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # If there\'s a pending command, process it with the new image\n            if self.pending_command:\n                self.process_command_with_image(self.pending_command)\n                self.pending_command = None\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {str(e)}\')\n\n    def voice_callback(self, msg):\n        """Process voice command"""\n        command = msg.data.lower()\n\n        # If we have a current image, process immediately\n        if self.current_image is not None:\n            self.process_command_with_image(command)\n        else:\n            # Store command for when image arrives\n            self.pending_command = command\n            self.get_logger().info(f\'Storing command for later processing: "{command}"\')\n\n    def process_command_with_image(self, command):\n        """Process voice command with current image context"""\n        # This is a simplified example - in practice, you\'d use multimodal models\n        # like CLIP, BLIP, or similar for grounding language in vision\n\n        if \'red ball\' in command:\n            # Look for red objects in the image\n            object_found = self.detect_red_object(self.current_image)\n            if object_found:\n                action_msg = String()\n                action_msg.data = f"go_to_object_red_ball_at_{object_found[\'position\']}"\n                self.action_pub.publish(action_msg)\n                self.get_logger().info(f\'Found red ball, sending navigation command\')\n\n        elif \'blue cup\' in command:\n            # Look for blue objects in the image\n            object_found = self.detect_blue_object(self.current_image)\n            if object_found:\n                action_msg = String()\n                action_msg.data = f"pick_up_object_blue_cup_at_{object_found[\'position\']}"\n                self.action_pub.publish(action_msg)\n                self.get_logger().info(f\'Found blue cup, sending pick-up command\')\n\n    def detect_red_object(self, image):\n        """Simple red object detection - in practice use proper computer vision"""\n        # Convert BGR to HSV for better color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Red can also be in the 170-180 range\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n\n        # Combine masks\n        mask = mask1 + mask2\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if contours:\n            # Find largest contour\n            largest_contour = max(contours, key=cv2.contourArea)\n            if cv2.contourArea(largest_contour) > 100:  # Minimum area threshold\n                # Calculate centroid\n                M = cv2.moments(largest_contour)\n                if M["m00"] != 0:\n                    cx = int(M["m10"] / M["m00"])\n                    cy = int(M["m01"] / M["m00"])\n                    return {\'position\': (cx, cy)}\n\n        return None\n\n    def detect_blue_object(self, image):\n        """Simple blue object detection"""\n        # Convert BGR to HSV\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for blue color\n        lower_blue = np.array([100, 50, 50])\n        upper_blue = np.array([130, 255, 255])\n        mask = cv2.inRange(hsv, lower_blue, upper_blue)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if contours:\n            # Find largest contour\n            largest_contour = max(contours, key=cv2.contourArea)\n            if cv2.contourArea(largest_contour) > 100:  # Minimum area threshold\n                # Calculate centroid\n                M = cv2.moments(largest_contour)\n                if M["m00"] != 0:\n                    cx = int(M["m10"] / M["m00"])\n                    cy = int(M["m01"] / M["m00"])\n                    return {\'position\': (cx, cy)}\n\n        return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vl_node = VisionLanguageNode()\n\n    try:\n        rclpy.spin(vl_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vl_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"4-human-robot-interaction-design",children:"4. Human-Robot Interaction Design"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Natural language command grammars"}),"\n",(0,t.jsx)(e.li,{children:"Feedback mechanisms and confirmation systems"}),"\n",(0,t.jsx)(e.li,{children:"Error handling and clarification requests"}),"\n",(0,t.jsx)(e.li,{children:"Safety considerations for voice-controlled systems"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"safe-voice-controlled-robot-interface",children:"Safe Voice-Controlled Robot Interface"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Safety-First Voice Interface:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport json\n\nclass SafeVoiceInterfaceNode(Node):\n\n    def __init__(self):\n        super().__init__('safe_voice_interface')\n\n        # Subscribe to processed voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            '/parsed_commands',\n            self.safe_command_callback,\n            10\n        )\n\n        # Publisher for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Publisher for safety warnings\n        self.warning_pub = self.create_publisher(String, '/safety_warnings', 10)\n\n        # Robot safety parameters\n        self.max_linear_speed = 0.5  # m/s\n        self.max_angular_speed = 0.5  # rad/s\n        self.emergency_stop_active = False\n\n        # Dangerous command patterns to filter\n        self.dangerous_patterns = [\n            'fast', 'quickly', 'hurry', 'run over', 'crash', 'break'\n        ]\n\n        self.get_logger().info('Safe Voice Interface Node initialized')\n\n    def safe_command_callback(self, msg):\n        \"\"\"Process commands with safety checks\"\"\"\n        command_data = json.loads(msg.data) if self.is_json(msg.data) else {'command': msg.data}\n\n        # Check if command is safe\n        if self.is_command_safe(command_data):\n            # Apply safety limits\n            safe_cmd = self.apply_safety_limits(command_data)\n            self.execute_safe_command(safe_cmd)\n        else:\n            warning_msg = String()\n            warning_msg.data = f\"SAFETY: Command '{command_data}' deemed unsafe and blocked\"\n            self.warning_pub.publish(warning_msg)\n            self.get_logger().warn(f'Blocked unsafe command: {command_data}')\n\n    def is_command_safe(self, command_data):\n        \"\"\"Check if command is safe to execute\"\"\"\n        cmd_str = str(command_data).lower()\n\n        # Check for dangerous patterns\n        for pattern in self.dangerous_patterns:\n            if pattern in cmd_str:\n                return False\n\n        # Check for speed-related commands that exceed limits\n        if any(speed_word in cmd_str for speed_word in ['full speed', 'maximum speed', 'top speed']):\n            return False\n\n        return True\n\n    def apply_safety_limits(self, command_data):\n        \"\"\"Apply safety limits to command\"\"\"\n        # This is a simplified example - in practice, you'd have more sophisticated safety logic\n        if isinstance(command_data, dict) and 'velocity' in command_data:\n            cmd = command_data.copy()\n            cmd['velocity']['linear']['x'] = min(cmd['velocity']['linear']['x'], self.max_linear_speed)\n            cmd['velocity']['angular']['z'] = min(cmd['velocity']['angular']['z'], self.max_angular_speed)\n            return cmd\n        return command_data\n\n    def execute_safe_command(self, command_data):\n        \"\"\"Execute command after safety verification\"\"\"\n        # Convert command to robot action\n        twist_cmd = self.convert_command_to_twist(command_data)\n        if twist_cmd:\n            self.cmd_vel_pub.publish(twist_cmd)\n            self.get_logger().info(f'Executing safe command: {command_data}')\n\n    def convert_command_to_twist(self, command_data):\n        \"\"\"Convert high-level command to Twist message\"\"\"\n        twist = Twist()\n\n        cmd_str = str(command_data).lower()\n        if 'forward' in cmd_str:\n            twist.linear.x = min(self.max_linear_speed, 0.3)\n        elif 'backward' in cmd_str:\n            twist.linear.x = max(-self.max_linear_speed, -0.3)\n        elif 'left' in cmd_str and 'turn' in cmd_str:\n            twist.angular.z = min(self.max_angular_speed, 0.3)\n        elif 'right' in cmd_str and 'turn' in cmd_str:\n            twist.angular.z = max(-self.max_angular_speed, -0.3)\n        elif 'stop' in cmd_str or 'halt' in cmd_str:\n            # Emergency stop - set all velocities to zero\n            pass  # Already zero by default\n\n        return twist\n\n    def is_json(self, myjson):\n        \"\"\"Check if string is valid JSON\"\"\"\n        try:\n            json.loads(myjson)\n        except ValueError:\n            return False\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    safety_node = SafeVoiceInterfaceNode()\n\n    try:\n        rclpy.spin(safety_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        safety_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"5-vla-model-integration",children:"5. VLA Model Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Connecting perception, language, and action systems"}),"\n",(0,t.jsx)(e.li,{children:"Real-time inference optimization"}),"\n",(0,t.jsx)(e.li,{children:"Latency considerations for interactive systems"}),"\n",(0,t.jsx)(e.li,{children:"Evaluation of VLA system performance"}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"complete-vla-integration-example",children:"Complete VLA Integration Example"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"VLA System Orchestrator:"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist, Pose\nimport threading\nimport time\n\nclass VLAOrchestratorNode(Node):\n\n    def __init__(self):\n        super().__init__('vla_orchestrator')\n\n        # Initialize all VLA components\n        self.initialize_components()\n\n        # Subscribe to all relevant topics\n        self.voice_sub = self.create_subscription(String, '/voice_commands', self.voice_callback, 10)\n        self.image_sub = self.create_subscription(Image, '/camera/rgb/image_raw', self.image_callback, 10)\n\n        # Publishers for robot control\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.nav_goal_pub = self.create_publisher(Pose, '/move_base_simple/goal', 10)\n\n        # Store context for multi-turn interactions\n        self.context_memory = {\n            'last_objects_seen': [],\n            'last_positions': [],\n            'current_intent': None\n        }\n\n        self.get_logger().info('VLA Orchestrator Node initialized')\n\n    def initialize_components(self):\n        \"\"\"Initialize all VLA system components\"\"\"\n        # In a real system, this would initialize Whisper, LLM, and perception models\n        self.whisper_initialized = True\n        self.llm_initialized = True\n        self.perception_initialized = True\n\n        self.get_logger().info('VLA components initialized')\n\n    def voice_callback(self, msg):\n        \"\"\"Handle voice command with full VLA pipeline\"\"\"\n        command = msg.data\n\n        # Process through cognitive planner with visual context\n        action_plan = self.generate_action_plan(command)\n\n        # Execute action plan with safety checks\n        self.execute_action_plan(action_plan)\n\n    def image_callback(self, msg):\n        \"\"\"Process visual input for context\"\"\"\n        # In a real system, this would run object detection and scene understanding\n        # For now, we'll just update context\n        self.context_memory['last_image_timestamp'] = time.time()\n\n    def generate_action_plan(self, command):\n        \"\"\"Generate action plan using VLA pipeline\"\"\"\n        # This is a simplified example - in practice, this would involve:\n        # 1. Language understanding (what does the user want?)\n        # 2. Visual grounding (where are relevant objects?)\n        # 3. Action planning (what sequence of actions to achieve goal?)\n\n        plan = {\n            'command': command,\n            'actions': [],\n            'confidence': 0.9  # High confidence for simple commands\n        }\n\n        # Simple rule-based planning - in practice, use LLM with visual context\n        if 'go to' in command or 'navigate to' in command:\n            # Extract destination and create navigation action\n            plan['actions'].append({\n                'type': 'navigation',\n                'target': self.extract_destination(command),\n                'priority': 1\n            })\n        elif 'pick up' in command or 'get' in command:\n            # Extract object and create manipulation action\n            plan['actions'].append({\n                'type': 'manipulation',\n                'object': self.extract_object(command),\n                'priority': 1\n            })\n        elif 'move' in command or 'go' in command:\n            # Create movement action\n            plan['actions'].append({\n                'type': 'movement',\n                'direction': self.extract_direction(command),\n                'distance': self.extract_distance(command),\n                'priority': 1\n            })\n\n        return plan\n\n    def execute_action_plan(self, plan):\n        \"\"\"Execute the generated action plan\"\"\"\n        for action in plan['actions']:\n            if action['type'] == 'navigation':\n                self.execute_navigation_action(action)\n            elif action['type'] == 'manipulation':\n                self.execute_manipulation_action(action)\n            elif action['type'] == 'movement':\n                self.execute_movement_action(action)\n\n    def extract_destination(self, command):\n        \"\"\"Extract destination from command\"\"\"\n        # Simple extraction - in practice, use NLP\n        if 'kitchen' in command:\n            return 'kitchen'\n        elif 'living room' in command:\n            return 'living_room'\n        elif 'bedroom' in command:\n            return 'bedroom'\n        else:\n            return 'unknown_location'\n\n    def extract_object(self, command):\n        \"\"\"Extract object to manipulate from command\"\"\"\n        # Simple extraction - in practice, use NLP\n        if 'ball' in command:\n            return 'ball'\n        elif 'cup' in command:\n            return 'cup'\n        elif 'book' in command:\n            return 'book'\n        else:\n            return 'unknown_object'\n\n    def extract_direction(self, command):\n        \"\"\"Extract movement direction from command\"\"\"\n        if 'forward' in command:\n            return 'forward'\n        elif 'backward' in command:\n            return 'backward'\n        elif 'left' in command:\n            return 'left'\n        elif 'right' in command:\n            return 'right'\n        else:\n            return 'forward'\n\n    def extract_distance(self, command):\n        \"\"\"Extract movement distance from command\"\"\"\n        # Simple extraction - in practice, use more sophisticated NLP\n        if 'meter' in command or 'm' in command:\n            return 1.0\n        else:\n            return 0.5\n\n    def execute_navigation_action(self, action):\n        \"\"\"Execute navigation action\"\"\"\n        # For this example, we'll use a predefined location\n        locations = {\n            'kitchen': {'x': 2.0, 'y': 1.0, 'theta': 0.0},\n            'living_room': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'bedroom': {'x': -1.0, 'y': 2.0, 'theta': 1.57}\n        }\n\n        target_loc = locations.get(action['target'])\n        if target_loc:\n            goal_pose = Pose()\n            goal_pose.position.x = target_loc['x']\n            goal_pose.position.y = target_loc['y']\n            goal_pose.position.z = 0.0\n            # Simple orientation\n            from math import sin, cos\n            half_theta = target_loc['theta'] / 2.0\n            goal_pose.orientation.z = sin(half_theta)\n            goal_pose.orientation.w = cos(half_theta)\n\n            self.nav_goal_pub.publish(goal_pose)\n            self.get_logger().info(f'Navigating to {action[\"target\"]}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_node = VLAOrchestratorNode()\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"technology-stack",children:"Technology Stack"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Voice Processing"}),": OpenAI Whisper (or equivalent open-source alternative)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language Models"}),": Open-source LLMs (e.g., Llama, Mistral, or similar)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Integration"}),": Python-based middleware with ROS 2"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception"}),": Combined with Isaac ROS and simulation outputs"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hardware"}),": Systems capable of running LLM inference (GPU recommended)"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,t.jsx)(e.h3,{id:"exercise-1-voice-command-recognition",children:"Exercise 1: Voice Command Recognition"}),"\n",(0,t.jsx)(e.p,{children:"Set up Whisper for voice command recognition:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Install Whisper and audio processing dependencies"}),"\n",(0,t.jsx)(e.li,{children:"Configure microphone input for real-time processing"}),"\n",(0,t.jsx)(e.li,{children:"Test voice recognition accuracy in different acoustic conditions"}),"\n",(0,t.jsx)(e.li,{children:"Integrate with ROS 2 messaging system"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-2-cognitive-planning-implementation",children:"Exercise 2: Cognitive Planning Implementation"}),"\n",(0,t.jsx)(e.p,{children:"Implement cognitive planning for robot tasks:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Create an LLM interface for task decomposition"}),"\n",(0,t.jsx)(e.li,{children:"Design prompt templates for different robot actions"}),"\n",(0,t.jsx)(e.li,{children:"Implement context management for multi-turn interactions"}),"\n",(0,t.jsx)(e.li,{children:"Test planning accuracy for various command types"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-3-vision-language-grounding",children:"Exercise 3: Vision-Language Grounding"}),"\n",(0,t.jsx)(e.p,{children:"Connect vision and language systems:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate camera feed with language understanding"}),"\n",(0,t.jsx)(e.li,{children:"Implement object detection for command grounding"}),"\n",(0,t.jsx)(e.li,{children:"Test spatial reasoning capabilities"}),"\n",(0,t.jsx)(e.li,{children:'Validate that robot correctly interprets "go to the red ball"'}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-4-safe-human-robot-interaction",children:"Exercise 4: Safe Human-Robot Interaction"}),"\n",(0,t.jsx)(e.p,{children:"Design safe voice-controlled robot system:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement safety filters for dangerous commands"}),"\n",(0,t.jsx)(e.li,{children:"Create confirmation mechanisms for critical actions"}),"\n",(0,t.jsx)(e.li,{children:"Test error handling and clarification requests"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate system performance with real human users"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,t.jsx)(e.p,{children:"Students will demonstrate proficiency by:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Creating a voice-controlled robot that responds to natural language commands"}),"\n",(0,t.jsx)(e.li,{children:"Implementing cognitive planning that decomposes complex tasks into executable actions"}),"\n",(0,t.jsx)(e.li,{children:"Integrating perception data to ground language commands in physical space"}),"\n",(0,t.jsx)(e.li,{children:"Demonstrating safe and effective human-robot interaction patterns"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"This module represents the synthesis of all previous modules, creating intelligent robotic systems capable of natural interaction with humans while performing complex tasks in physical environments."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>s});var o=i(6540);const t={},a=o.createContext(t);function r(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);